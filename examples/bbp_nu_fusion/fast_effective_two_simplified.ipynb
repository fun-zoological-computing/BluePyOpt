{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.7/site-packages/airspeed/__init__.py:505: FutureWarning: Possible nested set at position 8\n",
      "  KEYVALSEP = re.compile(r'[ \\t]*:[[ \\t]*(.*)$', re.S)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'make_allen_tests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d0cc93e5320a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmake_allen_tests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAllenTest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msciunit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mZScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'make_allen_tests'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Creating an optimisation with meta parameters\n",
    "# \n",
    "# This notebook will explain how to set up an optimisation that uses metaparameters (parameters that control other parameters)\n",
    "\n",
    "\n",
    "import bluepyopt as bpop\n",
    "import bluepyopt.ephys as ephys\n",
    "import pickle\n",
    "from sciunit.scores import ZScore\n",
    "from sciunit import TestSuite\n",
    "from sciunit.scores.collections import ScoreArray\n",
    "import sciunit\n",
    "import numpy as np\n",
    "from neuronunit.optimisation.optimization_management import dtc_to_rheo, switch_logic,active_values\n",
    "from neuronunit.tests.base import AMPL, DELAY, DURATION\n",
    "\n",
    "import quantities as pq\n",
    "PASSIVE_DURATION = 500.0*pq.ms\n",
    "PASSIVE_DELAY = 200.0*pq.ms\n",
    "import matplotlib.pyplot as plt\n",
    "from bluepyopt.ephys.models import ReducedCellModel\n",
    "import numpy\n",
    "from neuronunit.optimisation.optimization_management import test_all_objective_test\n",
    "from neuronunit.optimisation.optimization_management import check_binary_match, three_step_protocol,inject_and_plot_passive_model\n",
    "from neuronunit.optimisation.model_parameters import MODEL_PARAMS\n",
    "import copy\n",
    "\n",
    "#hof_to_euclid(hof,MODEL_PARAMS,target)\n",
    "MODEL_PARAMS[\"IZHI\"].pop('C',None)\n",
    "MODEL_PARAMS[\"IZHI\"].pop('k',None)\n",
    "MODEL_PARAMS[\"IZHI\"].pop('vPeak',None)\n",
    "MODEL_PARAMS[\"IZHI\"].pop('vt',None)\n",
    "MODEL_PARAMS[\"IZHI\"].pop('vr',None)\n",
    "MODEL_PARAMS[\"IZHI\"].pop('d',None)\n",
    "#sub_MODEL_PARAMS.pop('b',None)\n",
    "'''\n",
    "MODEL_PARAMS[\"IZHI\"].pop('C',None)\n",
    "MODEL_PARAMS[\"IZHI\"].pop('k',None)\n",
    "MODEL_PARAMS[\"IZHI\"].pop('vr',None)\n",
    "MODEL_PARAMS[\"IZHI\"].pop('vPeak',None)\n",
    "\n",
    "\n",
    "\n",
    "MODEL_PARAMS[\"IZHI\"].pop('vt',None)\n",
    "#MODEL_PARAMS[\"IZHI\"].pop('a',None)\n",
    "MODEL_PARAMS[\"IZHI\"].pop('d',None)\n",
    "'''\n",
    "#MODEL_PARAMS[\"IZHI\"].pop('c',None)\n",
    "\n",
    "#MODEL_PARAMS[\"IZHI\"].pop('C',None)\n",
    "#MODEL_PARAMS[\"IZHI\"].pop('k',None)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "simple_cell = ephys.models.ReducedCellModel(\n",
    "        name='simple_cell',\n",
    "        params=MODEL_PARAMS[\"IZHI\"],backend=\"IZHI\")  \n",
    "simple_cell.backend = \"IZHI\"\n",
    "\n",
    "\n",
    "# First we need to import the module that contains all the functionality to create electrical cell models\n",
    "\n",
    "# If you want to see a lot of information about the internals, \n",
    "# the verbose level can be set to 'debug' by commenting out\n",
    "# the following lines\n",
    "\n",
    "# Setting up the cell\n",
    "# ---------------------\n",
    "# This is very similar to the simplecell example in the directory above. For a more detailed explanation, look there.\n",
    "\n",
    "# For this example we will create two separate parameters to store the specific capacitance. One for the soma and one for the soma. We will put a metaparameter on top of these two to keep the value between soma and axon the same.\n",
    "\n",
    "# The metaparameter, the one that will be optimised, will make sure the two parameters above keep always the same value\n",
    "\n",
    "# And parameters that represent the maximal conductance of the sodium and potassium channels. These two parameters will be not be optimised but are frozen.\n",
    "\n",
    "# ### Creating the template\n",
    "# \n",
    "# To create the cell template, we pass all these objects to the constructor of the template.\n",
    "# We *only* put the metaparameter, not its subparameters.\n",
    "\n",
    "\n",
    "simple_cell = ephys.models.ReducedCellModel(\n",
    "        name='simple_cell',\n",
    "        params=MODEL_PARAMS[\"IZHI\"],backend=\"IZHI\")  \n",
    "simple_cell.backend = \"IZHI\"\n",
    "\n",
    "\n",
    "# Now we can print out a description of the cell\n",
    "\n",
    "model = simple_cell\n",
    "model.params = {k:np.mean(v) for k,v in model.params.items() }\n",
    "from make_allen_tests import AllenTest\n",
    "\n",
    "from sciunit.scores import ZScore\n",
    "tests = pickle.load(open(\"processed_multicellular_constraints.p\",\"rb\"))\n",
    "nu_tests = tests['Hippocampus CA1 pyramidal cell'].tests\n",
    "nu_tests[0].score_type = ZScore\n",
    "\n",
    "simulated_experiment = True\n",
    "features = None\n",
    "allen = True\n",
    "from collections.abc import Iterable\n",
    "\n",
    "if simulated_experiment:\n",
    "    while features is None:\n",
    "        aug_nu_tests, OM, target = test_all_objective_test(MODEL_PARAMS[\"IZHI\"],model_type=\"IZHI\",protocol={'allen':False,'elephant':True})\n",
    "        target = three_step_protocol(target)\n",
    "        if hasattr(target,'everything'):\n",
    "            features = copy.copy(target.everything)\n",
    "            \n",
    "            check_list = {key:feat for key,feat in features.items() if not feat is None and type(feat) is not type(Iterable) and feat !=0 and feat !=1 }\n",
    "            check_list.pop('depol_block_1.5x',None)\n",
    "            check_list.pop('is_not_stuck_3.0x',None)\n",
    "            check_list.pop('is_not_stuck_1.5x',None)\n",
    "            scale = 2.0/len(check_list)\n",
    "            if scale > 0.009999:\n",
    "                print('not high enough definition')\n",
    "                features = None\n",
    "                continue\n",
    "            \n",
    "            if allen:\n",
    "                simple_cell.allen = None\n",
    "                simple_cell.allen = True\n",
    "\n",
    "                tests = pickle.load(open(\"allen_NU_tests.p\",\"rb\"))\n",
    "                print('allen cell',tests[0].name)\n",
    "                nu_tests = tests[0]\n",
    "                for t in nu_tests:\n",
    "                    to_pop = []\n",
    "                    if t.name in features.keys():\n",
    "                        if features[t.name] is not None:\n",
    "                            t.set_observation(features[t.name])\n",
    "                            if t.observation['mean'] is not None:\n",
    "                                t.observation['std'] = np.abs(np.mean(t.observation['mean']))\n",
    "                        to_pop.append(t.name)\n",
    "                        \n",
    "                    [ features.pop(i,None) for i in to_pop ]\n",
    "                '''\n",
    "                more_tests = []\n",
    "                for feat in features:    \n",
    "                    if feat is not None:\n",
    "                        at = AllenTest(name=feat)\n",
    "                        at.set_observation(features[at.name])\n",
    "                        if at.observation['mean'] is not None:\n",
    "                            at.observation['std'] = np.abs(np.mean(t.observation['mean']))\n",
    "                            more_tests.append(at)\n",
    "                features = copy.copy(target.everything)\n",
    "                nu_tests = [t for t in nu_tests.tests]\n",
    "                nu_tests.extend(more_tests)\n",
    "                \n",
    "                '''\n",
    "sub_MODEL_PARAMS = copy.copy(MODEL_PARAMS['IZHI'])\n",
    "\n",
    "subset = list(sub_MODEL_PARAMS.keys())\n",
    "tg = target.dtc_to_gene(subset_params=subset)\n",
    "assert len(tg)==len(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = target.everything\n",
    "\n",
    "\n",
    "for key,feat in features.items():\n",
    "    if isinstance(feat, int):\n",
    "        print(key,feat)\n",
    "features\n",
    "nu_tests;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "big_list = []\n",
    "for tt in aug_nu_tests.values():\n",
    "    tt.core = None\n",
    "    tt.core = True\n",
    "    big_list.append(tt)\n",
    "big_list.extend(nu_tests)\n",
    "nu_tests = big_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#big_list = []\n",
    "#for tt in nu_tests:\n",
    "#    big_list.append(tt)\n",
    "#big_list.extend(more_tests)\n",
    "#nu_tests = big_list\n",
    "#    feature_name = '%s' % (tt.name)\n",
    "#    print(feature_name)\n",
    "import efel\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "list(efel.getFeatureNames());\n",
    "\n",
    "def basic_expVar(trace1, trace2):\n",
    "    # https://github.com/AllenInstitute/GLIF_Teeter_et_al_2018/blob/master/query_biophys/query_biophys_expVar.py\n",
    "    '''This is the fundamental calculation that is used in all different types of explained variation.  \n",
    "    At a basic level, the explained variance is calculated between two traces.  These traces can be PSTH's\n",
    "    or single spike trains that have been convolved with a kernel (in this case always a Gaussian)\n",
    "    Input:\n",
    "        trace 1 & 2:  1D numpy array containing values of the trace.  (This function requires numpy array\n",
    "                        to ensure that this is not a multidemensional list.)\n",
    "    Returns:\n",
    "        expVar:  float value of explained variance\n",
    "    '''\n",
    "    \n",
    "    var_trace1=np.var(trace1)\n",
    "    var_trace2=np.var(trace2)\n",
    "    var_trace1_minus_trace2=np.var(trace1-trace2)\n",
    "\n",
    "    if var_trace1_minus_trace2 == 0.0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return (var_trace1+var_trace2-var_trace1_minus_trace2)/(var_trace1+var_trace2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    " \n",
    "class NUFeatureAllen(object):\n",
    "    def __init__(self,test,model,cnt,target,check_list,spike_obs,print_stuff=False):\n",
    "        self.test = test\n",
    "        self.model = model\n",
    "        self.check_list = check_list\n",
    "        self.spike_obs = spike_obs\n",
    "        self.cnt = cnt\n",
    "        self.target = target\n",
    "        self.print_stuff = print_stuff\n",
    "    def calculate_score(self,responses):\n",
    "        if not 'features' in responses.keys():# or not 'model' in responses.keys():\n",
    "            return 100.0\n",
    "        check_list = self.check_list\n",
    "\n",
    "    \n",
    "        features = responses['features']\n",
    "\n",
    "        \n",
    "        feature_name = self.test.name\n",
    "        if feature_name not in features.keys():\n",
    "            return 100.0\n",
    "        \n",
    "        if features[feature_name] is None:\n",
    "            return 100.0\n",
    "            \n",
    "        if type(features[self.test.name]) is type(Iterable):\n",
    "            features[self.test.name] = np.mean(features[self.test.name])\n",
    "        self.test.observation['std'] = np.abs(np.mean(self.test.observation['mean']))\n",
    "        self.test.observation['mean'] = np.mean(self.test.observation['mean'])            \n",
    "        self.test.set_prediction(np.mean(features[self.test.name]))\n",
    "        \n",
    "        if 'Spikecount_3.0x'==feature_name or 'Spikecount_1.5x'==feature_name:\n",
    "            delta = np.abs(features[self.test.name]-np.mean(self.test.observation['mean']))\n",
    "\n",
    "            \n",
    "            return delta\n",
    "        else:\n",
    "            delta0 = np.abs(features['Spikecount_3.0x']-np.mean(self.spike_obs[0]['mean']))\n",
    "            delta1 = np.abs(features['Spikecount_1.5x']-np.mean(self.spike_obs[1]['mean']))\n",
    "            #print('should not get here')\n",
    "            #if self.print_stuff:\n",
    "            #    print(features['Spikecount_3.0x'],np.mean(self.spike_obs[0]['mean']))\n",
    "            #    print(features['Spikecount_1.5x'],np.mean(self.spike_obs[1]['mean']))\n",
    "\n",
    "\n",
    "            #else:\n",
    "\n",
    "            #if hasattr(self.test,'core'):\n",
    "            #    score = self.calculate_score_core(responses)\n",
    "            #    score = 0.000925*score\n",
    "            #    return score \n",
    "            '''\n",
    "            if self.cnt==287:# 286 spike_width2_3.0x\n",
    "                delta0 = 0.03*(1.0-float(basic_expVar(responses['dtc'].vm15,self.target.vm15)))\n",
    "                delta1 = 0.03*(1.0-float(basic_expVar(responses['dtc'].vm30,self.target.vm30)))\n",
    "                return delta0+delta1\n",
    "            if self.cnt==286:\n",
    "                delta = 0.03*(1.0-basic_expVar(responses['dtc'].vmrh,self.target.vmrh))\n",
    "                return float(delta)                \n",
    "            print(self.cnt,feature_name)\n",
    "            '''\n",
    "\n",
    "            if feature_name in check_list:\n",
    "                if features[feature_name] is None:\n",
    "                    return 100.0\n",
    "                score_gene = self.test.feature_judge()\n",
    "                if score_gene is not None:\n",
    "                    if score_gene.log_norm_score is not None:\n",
    "\n",
    "                        delta = np.abs(float(score_gene.log_norm_score))\n",
    "                        if delta==np.inf:\n",
    "                            if score_gene.raw is not None:\n",
    "                                delta =  np.abs(float(score_gene.raw))\n",
    "                    else:\n",
    "                        if score_gene.raw is not None:\n",
    "                            delta =  np.abs(float(score_gene.raw))\n",
    "                        else:\n",
    "                            delta = np.abs(features[feature_name]-np.mean(self.test.observation['mean']))\n",
    "                            \n",
    "                else:\n",
    "                    delta = np.abs(features[feature_name]-np.mean(self.test.observation['mean']))\n",
    "\n",
    "                if np.isnan(delta):\n",
    "\n",
    "                    delta = 100.0\n",
    "                if delta is None:\n",
    "\n",
    "                    delta = 100.0\n",
    "                score = 0.000001*delta\n",
    "               \n",
    "          \n",
    "                if 'AP_rise_rate_change_1.5x'==feature_name:\n",
    "                    score = score/10.0\n",
    "\n",
    "                if 'AP1_width_1.5'==feature_name:\n",
    "                    score = score/100.0\n",
    "                if 'AHP_depth_diff_1.5x'==feature_name:\n",
    "                    score = score/100.0\n",
    "                if 'AP2_AP1_begin_width_diff_1.5x'==feature_name:\n",
    "                    score = score/1000000000.0\n",
    "                if 'amp_drop_first_second_1.5x'==feature_name:\n",
    "                    score = score/1000000000.0\n",
    "\n",
    "                if 'AP_duration_half_width_change_3.0x'==feature_name:\n",
    "                    score = score/1000000000.0\n",
    "\n",
    "                if 'spike_width2_3.0x'==feature_name:\n",
    "                    score = score/1000000000.0\n",
    "\n",
    "                if 'AP_duration_half_width_change_3.0x'==feature_name:\n",
    "                    score = score/1000000000.0\n",
    "\n",
    "                if 'AP_duration_half_width_change_1.5x'==feature_name:\n",
    "                    score = score/10000000000.0\n",
    "                    \n",
    "\n",
    "                if 'AP_duration_change_3.0x'==feature_name:\n",
    "                    score = score/1000000000.0\n",
    "\n",
    "\n",
    "                if 'AP_duration_half_width_change_3.0x'==feature_name:\n",
    "                    score = score/1000000000.0\n",
    "                    \n",
    "                    \n",
    "                if 'AP_duration_half_width_change_1.5x'==feature_name:\n",
    "                    score = score/100.0\n",
    "                if 'decay_time_constant_after_stim_1.5x'==feature_name:\n",
    "                    score = score/100.0\n",
    "                if 'AP_duration_half_width_change_1.5x'==feature_name:\n",
    "                    score = score/100.0\n",
    "\n",
    "                #if score>1.0:\n",
    "                #    print(score,'hit',feature_name)\n",
    "                #    score = score/100.0\n",
    "                #    print(self.cnt)\n",
    "                return score+(delta0+delta1)\n",
    "\n",
    "        print(feature_name,'\\n\\n\\n\\n leaked case \\n\\n\\n\\n')\n",
    "\n",
    "objectives = []\n",
    "spike_obs = []#[None for i in range(0,1) ]\n",
    "for tt in nu_tests:\n",
    "    if 'Spikecount_3.0x' == tt.name:# or 'spike_width2_3.0x' in tt.name or 'AHP_time_from_peak_3.0x' in tt.name:# or 'isi' in tt.name or 'cv' in tt.name:\n",
    "        spike_obs.append(tt.observation)\n",
    "    if 'Spikecount_1.5x' == tt.name:\n",
    "        spike_obs.append(tt.observation)\n",
    "spike_obs = sorted(spike_obs, key=lambda k: k['mean'],reverse=True)\n",
    "\n",
    "for cnt,tt in enumerate(nu_tests):\n",
    "    feature_name = '%s' % (tt.name)\n",
    "    if feature_name in check_list:\n",
    "        if 'Spikecount_3.0x' == tt.name or 'Spikecount_1.5x' == tt.name:# or 'spike_width2_3.0x' in tt.name or 'AHP_time_from_peak_3.0x' in tt.name:# or 'isi' in tt.name or 'cv' in tt.name:\n",
    "            ft = NUFeatureAllen(tt,model,cnt,target,check_list,spike_obs)\n",
    "            objective = ephys.objectives.SingletonObjective(\n",
    "                feature_name,\n",
    "                ft)\n",
    "            print(cnt,feature_name)\n",
    "            objectives.append(objective)\n",
    "\n",
    "\n",
    "        \n",
    "score_calc = ephys.objectivescalculators.ObjectivesCalculator(objectives) \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# ### Creating the cell evaluator\n",
    "# \n",
    "# We will need an object that can use these objective definitions to calculate the scores from a protocol response. This is called a ScoreCalculator.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lop={}\n",
    "from bluepyopt.parameters import Parameter\n",
    "for k,v in MODEL_PARAMS[\"IZHI\"].items():\n",
    "    p = Parameter(name=k,bounds=v,frozen=False)\n",
    "    lop[k] = p\n",
    "\n",
    "print(lop)\n",
    "simple_cell.params = lop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Combining everything together we have a CellEvaluator. The CellEvaluator constructor has a field 'parameter_names' which contains the (ordered) list of names of the parameters that are used as input (and will be fitted later on).\n",
    "sweep_protocols = []\n",
    "for protocol_name, amplitude in [('step1', 0.05)]:\n",
    "\n",
    "    protocol = ephys.protocols.SweepProtocol(protocol_name, [None], [None])\n",
    "    sweep_protocols.append(protocol)\n",
    "twostep_protocol = ephys.protocols.SequenceProtocol('twostep', protocols=sweep_protocols)\n",
    "\n",
    "MODEL_PARAMS[\"IZHI\"]\n",
    "cell_evaluator = ephys.evaluators.CellEvaluator(\n",
    "        cell_model=simple_cell,\n",
    "        param_names=MODEL_PARAMS[\"IZHI\"].keys(),\n",
    "        fitness_protocols={twostep_protocol.name: twostep_protocol},\n",
    "        fitness_calculator=score_calc,\n",
    "        sim='euler')\n",
    "simple_cell.params_by_names(MODEL_PARAMS[\"IZHI\"].keys())\n",
    "simple_cell.params;\n",
    "\n",
    "\n",
    "\n",
    "# ## Setting up and running an optimisation\n",
    "# \n",
    "# Now that we have a cell template and an evaluator for this cell, we can set up an optimisation.\n",
    "\n",
    "\n",
    "MU = 100\n",
    "optimisation = bpop.optimisations.DEAPOptimisation(\n",
    "        evaluator=cell_evaluator,\n",
    "        offspring_size = MU,\n",
    "        map_function = dask_map_function,\n",
    "        selector_name='IBEA',mutpb=0.04,cxpb=0.15)\n",
    "\n",
    "final_pop, hall_of_fame, logs_early, hist_hearly = optimisation.run(max_ngen=150)#, cp_filename='results_100.p',continue_cp=False)\n",
    "\n",
    "gen_numbers = logs_early.select('gen')\n",
    "min_fitness = logs_early.select('min')\n",
    "max_fitness = logs_early.select('max')\n",
    "avg_fitness = logs_early.select('avg')\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(gen_numbers, max_fitness, label='max fitness')\n",
    "plt.plot(gen_numbers, avg_fitness, label='avg fitness')\n",
    "plt.plot(gen_numbers, min_fitness, label='min fitness')\n",
    "\n",
    "plt.xlabel('generation #')\n",
    "plt.ylabel('score (# std)')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(min(gen_numbers) - 1, max(gen_numbers) + 1) \n",
    "#plt.ylim(0.9*min(min_fitness), 1.1 * max(min_fitness)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_MODEL_PARAMS = copy.copy(MODEL_PARAMS['IZHI'])\n",
    "\n",
    "\n",
    "sub_MODEL_PARAMS\n",
    "hof_to_euclid(hall_of_fame,sub_MODEL_PARAMS,target)\n",
    "sub_MODEL_PARAMS\n",
    "hof_to_euclid(list(hist.genealogy_history.values()),sub_MODEL_PARAMS,target)\n",
    "subset = list(sub_MODEL_PARAMS.keys())\n",
    "tg = target.dtc_to_gene(subset_params=subset)\n",
    "tg\n",
    "MODEL_PARAMS['IZHI']\n",
    "tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectives2 = []\n",
    "for cnt,tt in enumerate(nu_tests):\n",
    "    feature_name = '%s' % (tt.name)\n",
    "    if feature_name in check_list:\n",
    "        ft = NUFeatureAllen(tt,model,cnt,target,check_list,spike_obs,print_stuff=True)\n",
    "        objective = ephys.objectives.SingletonObjective(\n",
    "            feature_name,\n",
    "            ft)\n",
    "        objectives2.append(objective)\n",
    "\n",
    "\n",
    "        \n",
    "score_calc2 = ephys.objectivescalculators.ObjectivesCalculator(objectives2) \n",
    "\n",
    "MODEL_PARAMS[\"IZHI\"]\n",
    "cell_evaluator2 = ephys.evaluators.CellEvaluator(\n",
    "        cell_model=simple_cell,\n",
    "        param_names=list(MODEL_PARAMS[\"IZHI\"].keys()),\n",
    "        fitness_protocols={twostep_protocol.name: twostep_protocol},\n",
    "        fitness_calculator=score_calc2,\n",
    "        sim='euler')\n",
    "simple_cell.params_by_names(MODEL_PARAMS[\"IZHI\"].keys())\n",
    "\n",
    "del optimisation.evaluator\n",
    "optimisation.evaluator = cell_evaluator2\n",
    "simple_cell.params;\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cp['population']\n",
    "#for ind in \n",
    "'''\n",
    "import pickle\n",
    "with open('evaluator.p','wb') as f:\n",
    "    pickle.dump(cell_evaluator2,f)\n",
    "\n",
    "with open('evaluator.p','rb') as f:\n",
    "    cell_evaluator2 = pickle.load(f)    \n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "MU = 100\n",
    "cp = {}\n",
    "cp['halloffame'] = hall_of_fame\n",
    "cp['population'] = final_pop\n",
    "#seed_pop = cp['halloffame']\n",
    "#cp = pickle.load(open('results_100.p', \"rb\"))\n",
    "\n",
    "#seed_pop.extend(cp['pop'])\n",
    "optimisation = bpop.optimisations.DEAPOptimisation(\n",
    "        evaluator=cell_evaluator2,\n",
    "        offspring_size = MU,\n",
    "        map_function = dask_map_function,\n",
    "        selector_name='IBEA',mutpb=0.045,cxpb=0.15,\n",
    "        seeded_pop=[cp['halloffame'],cp['population']])\n",
    "final_pop, hall_of_fame, logs, hist = optimisation.run(max_ngen=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# The optimisation has return us 4 objects: final population, hall of fame, statistical logs and history. \n",
    "# \n",
    "# The final population contains a list of tuples, with each tuple representing the two parameters of the model\n",
    "\n",
    "\n",
    "#print('Final population: ', final_pop)\n",
    "\n",
    "\n",
    "# The best individual found during the optimisation is the first individual of the hall of fame\n",
    "\n",
    "best_ind = hall_of_fame[0]\n",
    "#print('Best individual: ', best_ind)\n",
    "#print('Fitness values: ', best_ind.fitness.values)\n",
    "\n",
    "\n",
    "# We can evaluate this individual and make use of a convenience function of the cell evaluator to return us a dict of the parameters\n",
    "\n",
    "\n",
    "best_ind_dict = cell_evaluator.param_dict(best_ind)\n",
    "#print(cell_evaluator.evaluate_with_dicts(best_ind_dict))\n",
    "\n",
    "\n",
    "model = cell_evaluator.cell_model\n",
    "cell_evaluator.param_dict(best_ind)\n",
    "model.attrs = {str(k):float(v) for k,v in cell_evaluator.param_dict(best_ind).items()}\n",
    "\n",
    "\n",
    "\n",
    "opt = model.model_to_dtc()\n",
    "opt.attrs = {str(k):float(v) for k,v in cell_evaluator.param_dict(best_ind).items()}\n",
    "\n",
    "check_binary_match(target,opt)\n",
    "inject_and_plot_passive_model(opt,second=target)\n",
    "\n",
    "\n",
    "# As you can see the evaluation returns the same values as the fitness values provided by the optimisation output. \n",
    "# We can have a look at the responses now.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#plot_responses(twostep_protocol.run(cell_model=simple_cell, param_values=best_ind_dict, sim=nrn))\n",
    " \n",
    "\n",
    "\n",
    "# Let's have a look at the optimisation statistics.\n",
    "# We can plot the minimal score (sum of all objective scores) found in every optimisation. \n",
    "# The optimisation algorithm uses negative fitness scores, so we actually have to look at the maximum values log.\n",
    "\n",
    "gen_numbers = logs.select('gen')\n",
    "min_fitness = logs.select('min')\n",
    "max_fitness = logs.select('max')\n",
    "plt.plot(gen_numbers, min_fitness, label='min fitness')\n",
    "plt.xlabel('generation #')\n",
    "plt.ylabel('score (# std)')\n",
    "plt.legend()\n",
    "plt.xlim(min(gen_numbers) - 1, max(gen_numbers) + 1) \n",
    "plt.ylim(0.9*min(min_fitness), 1.1 * max(min_fitness)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "gen_numbers = logs.select('gen')\n",
    "min_fitness = logs.select('min')\n",
    "max_fitness = logs.select('max')\n",
    "avg_fitness = logs.select('avg')\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(gen_numbers, max_fitness, label='max fitness')\n",
    "plt.plot(gen_numbers, avg_fitness, label='avg fitness')\n",
    "plt.plot(gen_numbers, min_fitness, label='min fitness')\n",
    "\n",
    "plt.xlabel('generation #')\n",
    "plt.ylabel('score (# std)')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(min(gen_numbers) - 1, max(gen_numbers) + 1) \n",
    "#plt.ylim(0.9*min(min_fitness), 1.1 * max(min_fitness)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_numbers = logs.select('gen')\n",
    "min_fitness = logs.select('min')\n",
    "max_fitness = logs.select('max')\n",
    "avg_fitness = logs.select('avg')\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "#ax = fig.add_subplot(1,1)\n",
    "\n",
    "plt.plot(gen_numbers, min_fitness, label='min fitness')\n",
    "#ax.semilogy()\n",
    "#ax.set_yscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('generation #')\n",
    "plt.ylabel('score (# std)')\n",
    "plt.legend()\n",
    "plt.xlim(min(gen_numbers) - 1, max(gen_numbers) + 1) \n",
    "#plt.ylim(0.9*min(min_fitness), 1.1 * max(min_fitness)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inject_and_plot_passive_model(opt,second=target,plotly=False)\n",
    "best_ind_dict = cell_evaluator.param_dict(best_ind)\n",
    "objectives = cell_evaluator.evaluate_with_dicts(best_ind_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "best_ind_dict = cell_evaluator.param_dict(best_ind)\n",
    "objectives = cell_evaluator.evaluate_with_dicts(best_ind_dict)\n",
    "\n",
    "model = cell_evaluator.cell_model\n",
    "cell_evaluator.param_dict(best_ind)\n",
    "model.attrs = {str(k):float(v) for k,v in cell_evaluator.param_dict(best_ind).items()}\n",
    "\n",
    "opt = model.model_to_dtc()\n",
    "opt.attrs = {str(k):float(v) for k,v in cell_evaluator.param_dict(best_ind).items()}\n",
    "from neuronunit.optimisation.optimization_management import dtc_to_rheo, inject_and_plot_model30,check_bin_vm30,check_bin_vm15\n",
    "opt = dtc_to_rheo(opt)\n",
    "opt.rheobase\n",
    "opt.attrs;\n",
    "vm301,vm151,_,_,target = inject_and_plot_model30(target)\n",
    "vm302,vm152,_,_,opt = inject_and_plot_model30(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from neuronunit.optimisation.optimization_management import dtc_to_rheo, inject_and_plot_model30,check_bin_vm30,check_bin_vm15\n",
    "check_bin_vm30(target,opt)\n",
    "check_bin_vm15(target,opt)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def hof_to_euclid(hof,MODEL_PARAMS,target):\n",
    "    lengths = {}\n",
    "    tv = 1\n",
    "    cnt = 0\n",
    "    constellation0 = hof[0]\n",
    "    constellation1 = hof[1]\n",
    "    subset = list(sub_MODEL_PARAMS.keys())\n",
    "    tg = target.dtc_to_gene(subset_params=subset)\n",
    "    if len(MODEL_PARAMS)==1:\n",
    "        \n",
    "        ax = plt.subplot()\n",
    "        for k,v in MODEL_PARAMS.items():\n",
    "            lengths[k] = np.abs(np.abs(v[1])-np.abs(v[0]))\n",
    "\n",
    "            x = [h[cnt] for h in hof]\n",
    "            y = [0 for h in hof]\n",
    "            ax.set_xlim(v[0],v[1])\n",
    "            ax.set_xlabel(k)\n",
    "            tgene = tg[cnt]\n",
    "            yg = 0\n",
    "\n",
    "        ax.scatter(x, y, c='b', marker='o',label='samples')\n",
    "        ax.scatter(tgene, yg, c='r', marker='*',label='target')\n",
    "        ax.legend()\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    if len(MODEL_PARAMS)==2:\n",
    "        \n",
    "        ax = plt.subplot()\n",
    "        for k,v in MODEL_PARAMS.items():\n",
    "            lengths[k] = np.abs(np.abs(v[1])-np.abs(v[0]))\n",
    "                \n",
    "            if cnt==0:\n",
    "                tgenex = tg[cnt]\n",
    "                x = [h[cnt] for h in hof]\n",
    "                ax.set_xlim(v[0],v[1])\n",
    "                ax.set_xlabel(k)\n",
    "            if cnt==1:\n",
    "                tgeney = tg[cnt]\n",
    "\n",
    "                y = [h[cnt] for h in hof]\n",
    "                ax.set_ylim(v[0],v[1])\n",
    "                ax.set_ylabel(k)\n",
    "            cnt+=1\n",
    "\n",
    "        ax.scatter(x, y, c='r', marker='o',label='samples',s=5)\n",
    "        ax.scatter(tgenex, tgeney, c='b', marker='*',label='target',s=11)\n",
    "\n",
    "        ax.legend()\n",
    "\n",
    "        plt.show()\n",
    "    #print(len(MODEL_PARAMS))\n",
    "    if len(MODEL_PARAMS)==3:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        for k,v in MODEL_PARAMS.items():\n",
    "            lengths[k] = np.abs(np.abs(v[1])-np.abs(v[0]))\n",
    "        \n",
    "            if cnt==0:\n",
    "                tgenex = tg[cnt]\n",
    "\n",
    "                x = [h[cnt] for h in hof]\n",
    "                ax.set_xlim(v[0],v[1])\n",
    "                ax.set_xlabel(k)\n",
    "            if cnt==1:\n",
    "                tgeney = tg[cnt]\n",
    "\n",
    "                y = [h[cnt] for h in hof]\n",
    "                ax.set_ylim(v[0],v[1])\n",
    "                ax.set_ylabel(k)\n",
    "            if cnt==2:\n",
    "                tgenez = tg[cnt]\n",
    "\n",
    "                z = [h[cnt] for h in hof]\n",
    "                ax.set_zlim(v[0],v[1])\n",
    "                ax.set_zlabel(k)\n",
    "\n",
    "            cnt+=1\n",
    "        ax.scatter(x, y, z, c='r', marker='o')\n",
    "        ax.scatter(tgenex, tgeney,tgenez, c='b', marker='*',label='target',s=21)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "sub_MODEL_PARAMS = copy.copy(MODEL_PARAMS['IZHI'])\n",
    "\n",
    "\n",
    "sub_MODEL_PARAMS\n",
    "hof_to_euclid(hall_of_fame,sub_MODEL_PARAMS,target)\n",
    "sub_MODEL_PARAMS\n",
    "hof_to_euclid(final_pop,sub_MODEL_PARAMS,target)\n",
    "subset = list(sub_MODEL_PARAMS.keys())\n",
    "tg = target.dtc_to_gene(subset_params=subset)\n",
    "tg\n",
    "MODEL_PARAMS['IZHI']\n",
    "tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for gene in list(hist.genealogy_history.values()):\n",
    "hof_to_euclid(list(hist.genealogy_history.values()),sub_MODEL_PARAMS,target)\n",
    "sub_MODEL_PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_expVar(trace1, trace2):\n",
    "    # https://github.com/AllenInstitute/GLIF_Teeter_et_al_2018/blob/master/query_biophys/query_biophys_expVar.py\n",
    "    '''This is the fundamental calculation that is used in all different types of explained variation.  \n",
    "    At a basic level, the explained variance is calculated between two traces.  These traces can be PSTH's\n",
    "    or single spike trains that have been convolved with a kernel (in this case always a Gaussian)\n",
    "    Input:\n",
    "        trace 1 & 2:  1D numpy array containing values of the trace.  (This function requires numpy array\n",
    "                        to ensure that this is not a multidemensional list.)\n",
    "    Returns:\n",
    "        expVar:  float value of explained variance\n",
    "    '''\n",
    "    \n",
    "    var_trace1=np.var(trace1)\n",
    "    var_trace2=np.var(trace2)\n",
    "    var_trace1_minus_trace2=np.var(trace1-trace2)\n",
    "\n",
    "    if var_trace1_minus_trace2 == 0.0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return (var_trace1+var_trace2-var_trace1_minus_trace2)/(var_trace1+var_trace2)\n",
    "print(basic_expVar(target.vm15, opt.vm15))\n",
    "print(basic_expVar(target.vm30, opt.vm30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "opt = dtc_to_rheo(opt)\n",
    "print(opt.rheobase)\n",
    "print(target.rheobase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "check_binary_match(opt,target,plotly=False,snippets=False)\n",
    "check_binary_match(opt,target,plotly=False,snippets=True)\n",
    "print(basic_expVar(target.vmrh, opt.vmrh), 'variancce explained ratio at rheobase')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['injected_square_current'] = {}\n",
    "#if v.name in str('RestingPotentialTest'):\n",
    "params['injected_square_current']['delay'] = PASSIVE_DELAY\n",
    "params['injected_square_current']['duration'] = PASSIVE_DURATION\n",
    "params['injected_square_current']['amplitude'] = 0.0*pq.pA    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt_model = opt.dtc_to_model()\n",
    "opt_model.inject_square_current(params)\n",
    "opt_vm = opt_model.get_membrane_potential()\n",
    "opt_vm[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = target.dtc_to_model()\n",
    "target_model.inject_square_current(params)\n",
    "target_vm = target_model.get_membrane_potential()\n",
    "target_vm[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ind = hall_of_fame[1]\n",
    "#print('Best individual: ', best_ind)\n",
    "#print('Fitness values: ', best_ind.fitness.values)\n",
    "\n",
    "\n",
    "# We can evaluate this individual and make use of a convenience function of the cell evaluator to return us a dict of the parameters\n",
    "\n",
    "\n",
    "best_ind_dict = cell_evaluator.param_dict(best_ind)\n",
    "#print(cell_evaluator.evaluate_with_dicts(best_ind_dict))\n",
    "\n",
    "\n",
    "model = cell_evaluator.cell_model\n",
    "dtc= model.model_to_dtc()\n",
    "opt = dtc_to_rheo(opt)\n",
    "print(opt.rheobase)\n",
    "print(target.rheobase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.rheobase\n",
    "objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "best_ind_dict = cell_evaluator.param_dict(best_ind)\n",
    "objectives = cell_evaluator.evaluate_with_dicts(best_ind_dict)\n",
    "\n",
    "objectives2 = cell_evaluator2.evaluate_with_dicts(best_ind_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "logbook = logs\n",
    "#scores = [ m for m in logs ]\n",
    "'''\n",
    "list_of_dicts = []\n",
    "df1 = pd.DataFrame()\n",
    "genes=[]\n",
    "for _,v in hist.genealogy_history.items():\n",
    "    genes.append(v.fitness.values)\n",
    "for j,i in enumerate(objectives.keys()):\n",
    "    index = i.split('.')[0]\n",
    "    df1[str(index)] = pd.Series(genes).values[j]#, index=df1.index)\n",
    "\n",
    "'''\n",
    "#MU =14\n",
    "genes=[]\n",
    "min_per_generations = []\n",
    "for i,v in hist.genealogy_history.items():\n",
    "    if i%MU==0:\n",
    "        min_per_gen = sorted([(gene,np.min(gene)) for gene in genes],key=lambda x: x[1])\n",
    "        min_per_generations.append(min_per_gen[0][0])\n",
    "        genes =[]\n",
    "    genes.append(v.fitness.values)\n",
    "    \n",
    "df2 = pd.DataFrame()\n",
    "scores = []\n",
    "for j,i in enumerate(objectives.keys()):\n",
    "    index = i.split('.')[0]\n",
    "    print([i[j] for i in min_per_generations ])\n",
    "    df2[index] = pd.Series([i[j] for i in min_per_generations ])#, index=df1.index)\n",
    "df2    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number=int(np.sqrt(len(df2.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "box = int(np.sqrt(len(objectives)))\n",
    "fig,axes = plt.subplots(box,box+1,figsize=(20,20))#math.ceil(len(objectives)/2+1),figsize=(20,20))\n",
    "#axes[0,0].plot(scores)\n",
    "#axes[0,0].plot(gen_numbers, min_fitness, label='min fitness')\n",
    "\n",
    "axes[0,0].set_title('Observation/Prediction Disagreement')\n",
    "for i,c in enumerate(df2.columns):\n",
    "    ax = axes.flat[i+1]\n",
    "    history = df2[c]\n",
    "    #mn = mean[k.name] \n",
    "    #st = std[k.name] \n",
    "    #history = [(j[i]-mn)/st for j in scores ]\n",
    "    #ax.axhline(y=mn , xmin=0.02, xmax=0.99,color='red',label='best candidate sampled')\n",
    "\n",
    "    #ax.axvline(x=min_x , ymin=0.02, ymax=0.99,color='blue',label='best candidate sampled')\n",
    "    ax.plot(history)\n",
    "    ax.set_title(str(c))\n",
    "    #bigger = np.max([np.max(history),mn])\n",
    "    #smaller = np.max([np.min(history),mn])\n",
    "\n",
    "    #ax.set_ylim([np.min(history),np.max(history)])\n",
    "    #ax.set_ylabel(str(front[0].dtc.tests[i].observation['std'].units))\n",
    "axes[0,0].set_xlabel(\"Generation\")\n",
    "axes[0,0].set_ylabel(\"standardized error\")\n",
    "\n",
    "plt.tight_layout()\n",
    "#if figname is not None:\n",
    "#    plt.savefig(figname)\n",
    "#else:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logbook = logs\n",
    "#scores = [ m for m in logs ]\n",
    "list_of_dicts = []\n",
    "df1 = pd.DataFrame()\n",
    "genes=[]\n",
    "for _,v in hist.genealogy_history.items():\n",
    "    genes.append(v.fitness.values)\n",
    "for j,i in enumerate(objectives2.keys()):\n",
    "    index = i.split('.')[0]\n",
    "    df1[str(index)] = pd.Series(genes).values[j]#, index=df1.index)\n",
    "\n",
    "\n",
    "    \n",
    "#if normalize:\n",
    "#    a = (a - mean(a)) / (std(a) * len(a))\n",
    "#    v = (v - mean(v)) /  std(v)\n",
    "\n",
    "df1=(df1-df1.mean())/df1.std()\n",
    "\n",
    "corr = df1.corr()#.normalize()\n",
    "fig =plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns)#, annot=True)\n",
    "plt.show()\n",
    "print(np.sum(np.sum(corr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "logbook = logs\n",
    "#scores = [ m for m in logs ]\n",
    "list_of_dicts = []\n",
    "df1 = pd.DataFrame()\n",
    "genes=[]\n",
    "for _,v in hist.genealogy_history.items():\n",
    "    genes.append(v.fitness.values)\n",
    "for j,i in enumerate(objectives2.keys()):\n",
    "    index = i.split('.')[0]\n",
    "    df1[str(index)] = pd.Series(genes).values[j]#, index=df1.index)\n",
    "\n",
    "\n",
    "    \n",
    "#if normalize:\n",
    "#    a = (a - mean(a)) / (std(a) * len(a))\n",
    "#    v = (v - mean(v)) /  std(v)\n",
    "\n",
    "df1=(df1-df1.mean())/df1.std()\n",
    "\n",
    "corr = df1.corr()#.normalize()\n",
    "fig =plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns)#, annot=True)\n",
    "plt.show()\n",
    "print(np.sum(np.sum(corr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.genealogy_history.keys();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.genealogy_tree.keys();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "graph = networkx.DiGraph(hist.genealogy_tree)\n",
    "graph = graph.reverse()     # Make the graph top-down\n",
    "per_generation = [(gen,hist.genealogy_history[gen].fitness.values) for gen in graph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp['halloffame'][-1].fitness.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp['halloffame'][0].fitness.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp['population'][-1].fitness.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pop \n",
    "hall_of_fame[-1].fitness.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hall_of_fame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(hall_of_fame),0,-1)],[np.sum(i.fitness.values) for i in hall_of_fame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(0,len(hall_of_fame))],[np.sum(hall_of_fame[i].fitness.values) for i in range(len(hall_of_fame)-1,-1,-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "genes=[]\n",
    "\n",
    "for v in hall_of_fame:\n",
    "    #v = hall_of_fame[j]\n",
    "    genes.append(v.fitness.values)\n",
    "\n",
    "#for i in \n",
    "#    plt.plot([i for i in range(0,len(hall_of_fame))],[hall_of_fame[i].fitness.values[j] for i in range(len(hall_of_fame)-1,-1,-1)])\n",
    "#    plt.show()\n",
    "df1 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,i in enumerate(objectives.keys()):\n",
    "    index = i.split('.')[0]\n",
    "    df1[str(index)] = pd.Series(genes).values[j]#, index=df1.index)\n",
    "\n",
    "df1=(df1-df1.mean())/df1.std()\n",
    "\n",
    "corr = df1.corr()\n",
    "fig =plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns)#, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "for j,i in enumerate(objectives2.keys()):\n",
    "    index = i.split('.')[0]\n",
    "    df1[str(index)] = pd.Series(genes).values[j]#, index=df1.index)\n",
    "\n",
    "df1=(df1-df1.mean())/df1.std()\n",
    "\n",
    "corr = df1.corr()\n",
    "fig =plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns)#, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
